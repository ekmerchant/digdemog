<!--

document.getElementById("pcontent")
.innerHTML =
'Topic modeling is one of several methods of <a class="content" href="http://www.digitalhumanities.org/dhq/vol/8/1/000171/000171.html">distant reading</a> recently developed by scholars in the humanities. Based on the statistical method <a class="content" href="http://www.matthewjockers.net/2011/09/29/the-lda-buffet-is-now-open-or-latent-dirichlet-allocation-for-english-majors/">latent dirichlet allocation</a> (LDA), topic modeling is an algorithmic method of identifying patterns of word (or phrase) co-occurrence across large textual corpora. \
<p>LDA is a generative model, that is, a model that randomly generates data, in which users tune specific parameters to discover the conditions that generate the observed data. Topic modeling assumes that, consciously or not, writers construct their texts by drawing from culturally-available distributions of words. We refer to these lexical distributions as "topics," though these "topics" do not exist in the world as recognizable objects. For example, an article on sandwiches might draw 40% of its text from a lexical distribution in which "wheat," "yeast," and "fermentation" occur much more frequently than "meat," and another 35% from a separate lexical distribution in which "meat," "cheese," and "mustard" occur much more frequently than "yeast." These lexical distributions contain the same words (the entire English language), but are distinguished by the frequency with which the words appear. The LDA algorithm assigns words from a given corpus to a user-specified number of topics, iterating successively (and reassigning words) to increase the likelihood that the lexical distributions it has created could have generated the observed corpus. These distributions or topics are often referred to as "bags of words" because they specify the relative frequency with which words occur in a corpus, but the order in which the words occur in the documents that comprise the corpus is ignored. The final results of a topic modeling exercise include the final lexical distributions (topics) and, for each document in the corpus, the proportion generated from each topic. That is, given a number of topics specified by the user, the algorithm returns that many "bags of words" (defined by the frequency with which words occur in the bag) and, for each text in the corpus, what proportion of the text was drawn (in the generative model) from each bag (topic). \
<p>In the sandwich example, we might label the first topic "bread" and the second topic "sandwich filling," but these topics do not exist as such external to our corpus. That is, if we were to expand the corpus, particularly to include discussions of sandwiches over a span of centuries, we would not expect all discussions of bread and sandwich fillings to use the same words in the same distributions as our original article used. The topics "discovered" by LDA therefore only give us information about our corpus, not about external reality. Nonetheless, topic modeling can be very useful for identifying patterns of word (or phrase) co-occurrence within corpora (however defined) that might not be readily apparent through close reading or in corpora that are too large to be read closely.\
<p>The corpus under analysis in this project includes all research articles published through 2010 in the three general English-language demography journals established in the second half of the twentieth century: <i>Population Studies</i>, established in 1947 by the British Population Investigation Committee; <i>Demography</i>, established in 1964 by the Population Association of America, and <i>Population and Development Review</i>, established in 1975 by the Population Council. The full text of these journals was provided to me by <a class="content" href="http://www.jstor.org">JSTOR</a> through its <a class="content" href="http://dfr.jstor.org">Data for Research</a> program. I wrote a <a class="content" href="https://www.python.org">Python</a> program to prepare the documents for analysis by removing non-alphabetical characters and stopwords (words that appear frequently but contain no meaning, such as "a" and "the"). I carried out the topic modeling in <a class="content" href="http://mallet.cs.umass.edu">MALLET</a> via <a class="content" href="http://www.r-project.org">R</a>, as described in <a class="content" href="http://www.themacroscope.org/?page_id=67">The Historian\'s Macroscope</a>, specifying 30 topics. There is no single objective criterion for selecting the number of topics in a topic model, though optimizing log likelihood is one desideratum. I experimented with as few as 15 and as many as 50 topics, and found that 30 optimized both log likelihood and intelligibility. I used R and Python to process the results and designed visualizations using the <a class="content" href="http://d3js.org">D3</a> library in JavaScript.\
<p>Results are presented in three sections. The <a class="content" href="alltopics.html">All Topics</a> page shows, for each topic identified by LDA (labeled by number so as to avoid imposing my own reading on the results), a word cloud indicating the relative frequency of words in the distribution and a graph over time of the proportion of the corpus accounted for by that topic. Clicking the link for any topic will take you to the <a class="content" href="topic1.html">Articles by Topic</a> section of the site. Here there is a page for each topic (use the arrows at the top of the page to navigate between topics), showing the same word cloud presented in the All Topics page, a graph indicating the number of articles corresponding to the topic over time and by journal (for this graph, an article composed 50% of topic one will contribute .5 of an article to the graph, for example), and a list of the articles in each year that best represent the given topic, also showing the proportion of the article accounted for by that topic and by the (other) two most prevalent topics in the article. For example, the first article listed on the page for <a class="content" href="topic1.html">Topic 1</a>, "The Sources of Indian Emigration to Fiji" by K.L. Gillion, is the article with the highest proportion of text from Topic 1 in 1956. However, as shown in the column on the right, only 23% of the article is drawn from Topic 1, with 48% drawn from <a class="content" href="topic9.html">Topic 9</a> and 20% from <a class="content" href="topic22.html">Topic 22</a>. The remaining 9% of the article is drawn from the other 27 topics. The titles in the list link to the text of the articles in JSTOR. The page <a class="content" href="topjournal.html">Topics by Journal</a> groups topics into categories reflecting themes in population science and graphs the prevalence of those categories by journal over time. The categories used here are Migration (Topics 10 and 18), Fertility (Topics 8, 12, 15, and 16), Mortality (Topics 11, 23, and 24), Quantitative Analysis (Topics 3, 6, and 7), Surveys and Censuses (Topics 21 and 28), Theory (Topic 22), Family Structure (Topics 17 and 26), Demographic Transition (Topics 2, 9, and 20), Social Change and Inequality (Topics 13, 19, and 27), Global South (Topics 1, 4, 14, and 25), Economic Development (Topics 29 and 30), and Eugenics and Biodemography (Topic 5). These categories represent my own judgment of what the lexical distribution for each topic signifies, and another analyst might have classified the topics into very different categories. All three graphs are on the same scale, but the graphs for <i>Demography</i> and <i>Population and Development Review</i> are blank in the period before those journals were established. All graphs are smoothed using a three-year moving average and the quantities are proportions, so they sum to 1. The <a class="content" href="topicexp.html">Discussion of Topic Analysis</a> page examines the results presented on the preceding pages in greater detail.'
    
-->